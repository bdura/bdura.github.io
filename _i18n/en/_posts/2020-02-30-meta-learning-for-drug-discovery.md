---
date: 2020-02-30
title: Meta-learning for drug discovery
categories:
  - ai
  - pharma
image: https://source.unsplash.com/0tfz7ZoXaWc
image-caption: Haha
---

Drug discovery today is a long, costly and arduous process. On average, it costs around one-and-a-half billion dollars to run a drug discovery campaign, which takes over ten years to complete. After impressive results in a range of applications and ever more powerful algorithms being developed, artificial intelligence might be the key to accelerate the entire process, finding treatment to orphan disease along the way.

Traditionally, drug discovery has relied on the mere magnitude of the scope, testing thousands of compounds at the same time and with very little prior idea of the results, in order to stumble upon the handful of molecules that can move on clinical trials â€“with no guarantee of success.

How do you evaluate the potential of a novel molecule regarding a particular task? To put it simply, until very recently, you did not. Drug generation was and still is a long and arduous process, wherein you need to actually test each of your candidate molecules _in vitro_ to have any idea whether or not they can have a positive impact on human health.

This is not a fatality. Fast-forward to today, and computational drug discovery is blossoming, with the promise of substantially expanding the scope of problems that can be answered and reducing the time and costs associated with solving them. Machine-learning has the potential to give us the prospect of a new molecule without any actual experiment, and one can even dream of an algorithm able to generate novel compounds perfectly tailored to attack a given disease.

To make such promise a reality, artificial intelligence still has a long way to go. Data efficiency, a long goal for machine-learning scientists, is still out of reach and yet paramount in a fundamentally low-data problem such as drug discovery.

At InVivo AI, I worked alongside great talents to find answers to these formidable challenges, by applying myself to fundamental science in artificial intelligence. The crux of my contribution in that direction involved working on a novel family of algorithms, which we called "Adaptive Deep Kernel Learning", or ADKL for short. ADKL aims to tackle some of the limitations of modern machine-learning algorithms with a principled method.
